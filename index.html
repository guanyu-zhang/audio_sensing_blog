<!DOCTYPE HTML>
  <html>
    <head>
        <meta http-equiv="Content-type" content="text/html;  charset=UTF-8" />
        <title>Audio Sensing</title>
        <meta http-equiv="Content-Language" content="en-us"  />
        <meta http-equiv="imagetoolbar" content="no"  />
        <meta name="MSSmartTagsPreventParsing"  content="true" />
        <meta name="description" content="Description"  />
        <meta name="keywords" content="Keywords"  />
        <meta name="author" content="gyz"  />
        <style type="text/css"  media="all">@import  "css/master.css";</style>
        <link rel="shortcut icon" href="favicon.ico">
    </head>
    <body>
        <div id="page-container">
            <div id="main-nav">
                <ul>
                    <li id="TEST"><a href="https://google.com" ><font size = 5>Test</font></a></li> 
                    <li id="DATA"><a href="https://github.com/guanyu-zhang/ee382v_final_proj" ><font size = 5>Data</font></a></li> 
                    <li id="REPORT"><a href="https://guanyu-zhang.github.io/audio_sensing_blog/" ><font size = 5>Report</font></a></li>
                </ul>
 
            </div>
            <div  id="header">
            </div>

            <div  id="content">
                    <p> <em><i>Contributers:</i></em>&nbsp; Dawei Liang, Guanyu Zhang</p> <br/><br/>
                    <h2>Backgrounds and Motivations</h2>

                    <p style="text-align:justify"; text-justify:inter-ideograph;>Human speech collected from commercial sensors can be insightful for applications such as automated social diary [1] or analysis of people’s social interaction behaviors [2]. However, reliable modeling of the speech data has to rely on solid ground truth of the data. A common problem in real-world collected speech is the presence of ambiguous speech snippets or mixture of voice within the same instance. The situation is relatively mild in voice classification studies with large-scale online datasets [3], but it is more significant in naturalistic studies where the ‘clean’ speech is smaller with a much higher proportion of the outliers and overlap speech. Hence, our project aims to reduce such uncertainty for audio classification with the real-life collected data.
                    </p>

                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                        Considering that the annotation process of the human speech data is time-consuming and is not always reliable, especially if the target voice (speech data from the target speakers) is overlapped with the background voice or if the target speakers have similar voice patterns as other speakers. Through our project we hope to find a way to simplify the annotation process and build a model to approach the manual classification results. We will see in the following part that prior efforts of overlapped speech detection do not take into account the target speech. They were aimed to tell whether there is overlapped speech, while our goal is to tell whether the overlaps are composed of the target speech or not.
                    </p>
                    <br/><br/>
                    <h2>Descriptions and Dataset</h2>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>The goal of our project is to build an easy yet reliable method for the determination of outliers and mixture speech in the speech sensing data. Mixture speech in our study is defined as a speech snippet containing voice elements from more than one speaker. The voice elements can either be separate or overlapped with each other. An outlier in our study is defined as a speech snippet that was not able to be annotated by human listeners where the voice pattern can be either ambiguous or similar to multiple speakers’ voice. This is typically caused by far-field recording of the sounds or speech generated by two simultaneous speakers with very similar voice, accent, and speaking style.
                    </p>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>We plan to use model-based methods as we have pre-labeled training and test sets for the development. Prior work has shown the capabilities of deep learning on overlap speech counting [4], or in general, overlap sound classification [5]. Hence, we plan to start with simple neural networks to see how it performs.
                    </p>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>The dataset we are going to use was originally obtained from a real-world data collection effort using commercial wearable smartwatch. 
                        The data was collected in a naturalistic home environment, where people were instructed to perform some types of interactions. In total, we collected 41,973 seconds (11.7 hours) of audio recordings from 7 groups of participants for this study.
                        Due to privacy protection reasons, we only have access to spatial acoustic features (MFCC and loudness) and deep embedding features extracted from a pre-trained neural network. Also, due to the resource limit, we only accessed a subset of the dataset for our model development and validation.  
                    </p>
                    <figure>
                        <img src='images/watch.jpg' alt='watch'>
                        <figcaption>The Fossil smartwatch for our data collection</figcaption>
                    </figure>
                    <br/>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>The data was labeled by human labelers ahead of the study. It was categorized into 4 classes.
                        Here we have four types of labels, the first type is pure speech from the smartwatch wearers. That is also what we refer as the target speakers. The second type is the background voice. It is basically voice from speakers other than the target speakers. The third and the fourth types are what we are going to filter from the dataset. The third type is segments of mixed voice of the target and non-target speakers. And the fourth type contains speech instances that are hard to identify by human listeners. Just as we showed in the previous slides, this is typically because the target and non-target speakers are too similar in voice, or if the conversation happens too far away that are not clear enough.
                        However, due to the significant amount of original audio size and error of human inputs, the data may be only weakly labeled. The weak label characteristics affected our recognition performance, and we can see that in the following part.                    
                    </p>
                    <figure>
                        <table>
                            <tr>
                                <td>
                                    <img src=images/labels_total.png alt="labels distribution">
                                </td>
                                <td>
                                    <img src=images/labels_pie.png alt="labels distribution">
                                </td>
                            </tr>
                         </table>
                    </figure>
                    <figcaption>labels distribution</figcaption>
                    <br/><br/><br/>
                    <h2>Related Work</h2>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    The overall picture of our work is to facilitate foreground speech detection with real-world collected audio data. Prior work of foreground speech detection typically does not focus much on the outliers and the mixed speech segments of the data [6]. However, they may bring uncertainty to the model performance, especially in the case of intensive conversations.  There has been prior work discussing sound classification with mixed background or components. For example, in [5] the researchers attempted to classify sound with different environmental backgrounds. Neural networks have also been shown to work well for speaker or component counting within the sound segments [4]. However, in general there is still limited work discussing the determination of such sound instances from real-world audio sensing data, and we hope to explore a feasible solution for our overall study.
                    </p>
                    <br/><br/>
                    <h2>Models Description</h2>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    We tried two types of methods to determine the uncertain speech instances. The first method is a straight forward end-to-end classification. We tried a random forest, and a more advanced VGG-like NN classifier. The second method is actually a bit different. So rather than obtaining the predicted labels from the classifiers, we directly examine the predicted probability from the classifiers. 
                    The VGG-S [7] NN classifier has the following structure:
                    </p>
                    <p align='center' > <font size=3> Input => Conv1D[64] => Conv1D[128] => Conv1D[256] => Conv1D[512] => Dense[1024] => Dense[128] => Dense[1]</font>
                    <br/><font size=3>*The numbers in the brackets are the number of filters for Conv layers and the size of outputs for Dense layers.</font>
                    </p>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                        In the training phase, everything is basically the same as before. We got training data and labels, and then we trained a classifier. But in the test phase, we can actually test the model with classes that it has never seen in the training phase. We determine the instances of such classes by thresholding the predicted probability or confidence levels from the classifier (Considering that the final output of the VGG-S NN is a scalar and there are three categories to be classified).
                    This process is shown below.
                    </p>
                    <figure>
                        <table>
                            <tr>
                                <td>
                                    <img src=images/models_selection1.png alt="Train phase">
                                    <figcaption>Training phase</figcaption>
                                </td>
                                <td>
                                    <img src=images/models_selection2.png alt="Test phase">
                                    <figcaption>Test phase</figcaption>
                                </td>
                            </tr>
                         </table>
                    </figure>
                    <br/>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    In addition, we also do some enhancement for the input features. The first way is we get the running average of every few seconds of the frames. The second way is that we stack the frames, so that they can be image-like inputs. By doing so, the input instances can incorporate the temporal shape of the speech. 
                    </p>
                    <figure>
                        <table>
                            <tr>
                                <td>
                                    <img src=images/windowing1.png alt="Mean texture window">
                                    <figcaption>Mean texture window</figcaption>
                                </td>
                                <td>
                                    <img src=images/windowing2.png alt="Stacked image-like feature">
                                    <figcaption>Stacked image-like feature</figcaption>
                                </td>
                            </tr>
                         </table>
                    </figure>
                    <br/><br/><br/>
                    <h2>Results</h2>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    We first examine how we can determine the speech mixtures from the dataset. As we can see, by using a NN classifier, it is possible to directly discover the mixed conversational segments from the dataset. Neural networks are generally performing better for this task.
                    </p>
                    <figure>
                        <img src='images/results_mixed_tab1.png' alt='mixture result 1'>
                        <figcaption>Binary classification, background voice vs mixed (3-sec instance)</figcaption>
                    </figure>
                    <br/>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    From the table above we can see that the mixed speech instances can be determined directly from the background voice by using end-to-end classification.
                    We also know that NN classifiers tend to perform better for the classification.
                    </p>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    We further examine how the different types of input enhancement can change the performance. The general conclusion is, either type of feature enhancement can yield similar performance. It looks like stacking the frames doesn’t really help a lot.
                    </p>
                    <figure>
                        <img src='images/results_mixed_tab2.png' alt='mixture result 2'>
                        <figcaption>Binary classification with different input types, background vs mixed (3-sec instance)
                        </figcaption>
                    </figure>
                    <br/>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    The third part of the results is about how the input length of the speech segments can change the performance. The unit size of the segments is in 1 sec. As we can see, by increasing the size of the input speech segments, the classification performance generally goes up for both classifiers.
                    </p>
                    <figure>
                        <img src='images/results_mixed_tab3.png' alt='mixture result 3'>
                        <figcaption>The relationship between temporal size and F1 score
                        </figcaption>
                    </figure>
                    <br/>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                        Then, we trained models to process speech data from the uncertain speakers. Here our main goal is to build a model that can classify the following three categories of speech data. The voice from the target speaker, the background voice (which is the same thing as the far-field vocal backgrounds  that we defined before) and the uncertain speech data (which contains different speakers or speakers with similar voice patterns). We first tried an end-to-end model (still used the VGG-S structure). However, the model did not perform well and we got a F1 score around 58% and the balanced accuracy is around 60%. We noticed that it might be difficult to directly classify the three categories but we found that if we use the model trained with only target speakers and background voice. The output of the three categories seems to show some specific pattern. As we can see in the table below. The output of target speakers is around 1, the output of background voice is around 0 and the output of uncertain voice is in the middle. That means speech instances of uncertain speakers may not be classified in an end-to-end approach, but can be determined by thresholding the output confidence of a voice classifier.
                    </p>
                    <figure>
                        <img src='images/results_uncertain_tab.png' alt='uncertain result'>
                        <figcaption>Confidence levels of each categorie (5 times results using VGG-S)
                        </figcaption>
                    </figure>
                    <br/><br/><br/>
                    <h2>Conclusions and Future Directions</h2>
                    <p style="text-align:justify"; text-justify:inter-ideograph;>
                    First, we build an end-to-end model that can directly distinguish the target speakers data and the mixture speech data. However, Speech instances of uncertain speakers may not be classified from the dataset directly, but they can be discovered based on the output confidence of a voice classifier.
                    In the future, we will work to build a more systematic and integral pipeline to determine such uncertain instances. We will also try to improve the performance of the models especially about classifying background voice and uncertain voice.
                    </p>
                    <h3>Contact  Us</h3>
                        Dawei Liang  &nbsp  &nbsp Website: <a href="https://github.com/dawei-liang"> https://github.com/dawei-liang</a> <br/>
                        Guanyu zhang  &nbsp Website: <a href="https://github.com/guanyu-zhang">https://github.com/guanyu-zhang</a>
                        <!-- Email: <a href="mailto:"  >info@enlighten.co.nz</a><br/> -->
                        
            </div>
            <div  id="footer">
                <div  id="altnav">
                    <font size=2><a href="https://github.com/guanyu-zhang"  >Contact Us</a> </font>
                </div>
            <!-- Copyright © Dawei Liang <br> -->
            <font size = 2>Powered by <a  href="https://pages.github.com" >Github Pages</a> </font>
            </div>
        </div>
    </body>
</html>